{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scoobiii/DeepEnergy/blob/master/Inmemory_optimize_energy_consumption_using_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deep_memory_\n"
      ],
      "metadata": {
        "id": "nFZniS62DG4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o script de inicialização\n",
        "script = \"\"\"\n",
        "#!/bin/bash\n",
        "# Cria o diretório /mnt/ramdisk\n",
        "mkdir -p /mnt/ramdisk\n",
        "\n",
        "# Monta o ramdisk de 10 GB utilizando tmpfs\n",
        "mount -t tmpfs -o size=10G tmpfs /mnt/ramdisk\n",
        "\n",
        "# Verifica se o ramdisk foi montado corretamente\n",
        "df -h /mnt/ramdisk\n",
        "\"\"\"\n",
        "\n",
        "# Escrever o script no arquivo /root/mount_ramdisk.sh\n",
        "with open('/root/mount_ramdisk.sh', 'w') as f:\n",
        "    f.write(script)\n",
        "\n",
        "# Torna o script executável\n",
        "!chmod +x /root/mount_ramdisk.sh\n"
      ],
      "metadata": {
        "id": "q-ldosw4DUW2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K7E8oiQozF1i"
      },
      "outputs": [],
      "source": [
        "# Artificial Intelligence for Business\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueRswAHpzF1p"
      },
      "source": [
        "# Building the environment\n",
        "- Use a class to create different Environment objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P2MotQpxzF1s"
      },
      "outputs": [],
      "source": [
        "class Environment(object):\n",
        "    # introduce and initialize all paramaters and variables of the environment\n",
        "    def __init__(self, optimal_temperature = [18.0, 24.0], initial_month= 0, \\\n",
        "                 initial_number_users = 10, initial_rate_data = 60):\n",
        "\n",
        "        self.initial_month = initial_month\n",
        "\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0,\n",
        "                                                 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
        "                                    + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0 # mid of optimal range\n",
        "\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1        # train or inference mode\n",
        "\n",
        "    # method to update environment after AI plays an action\n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        \"\"\" variables:\n",
        "         - direction :  change of temperature by AI incr or decr +1 or -1 \"\"\"\n",
        "\n",
        "        # GETTING THE REWARD\n",
        "        # Computing the energy spent by the server's cooling system when there is no AI\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward and Scaling the Reward\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        self.reward = 1e-3 * self.reward\n",
        "\n",
        "        # GETTING NEXT STATE\n",
        "        # Updating the atmospheric temperature\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users between the min / max range\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data between the min / max range\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature       # T° of server before action\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
        "                                     + 1.25 * self.current_rate_data  # T° of server updated\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI action\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai  # energy cost = abs delta of T° change by assumption\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server's Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server's Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "\n",
        "        # GETTING GAME OVER (allows to end of an epoch if T° out of bound during training)\n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if self.train == 1:\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if self.train == 1:\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "\n",
        "        # UPDATING THE SCORES\n",
        "        self.total_energy_ai += energy_ai\n",
        "        self.total_energy_noai += energy_noai\n",
        "\n",
        "        # NORMALIZE NEXT STATE (state vector to be fed to neural network)\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / \\\n",
        "                                (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / \\\n",
        "                              (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / \\\n",
        "                           (self.max_rate_data - self.min_rate_data)\n",
        "        # create vector for updated state\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "\n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "    # METHOD THAT RESETS THE ENVIRONMENT\n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
        "                                     + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # METHOD PROVIDING CURRENT STATE, LAST REWARD AND WHETHER THE GAME IS OVER\n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / \\\n",
        "                                (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / \\\n",
        "                              (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / \\\n",
        "                           (self.max_rate_data - self.min_rate_data)\n",
        "        # calc vector of current state\n",
        "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "\n",
        "        return current_state, self.reward, self.game_over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PqF-lnZzF1w"
      },
      "source": [
        "# Building the Neural network\n",
        "- Fully connected NN with 2 hidden layers (64 then 32 nodes)\n",
        "- Input : state vector (server T°, number of users, rate of data)\n",
        "- Output : Q-values of AI actions to regulate T° ( reduce by 3°C or 1.5°, maintain T°, Incr. by 1.5° or 3°C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fcYuoJKmzF1y"
      },
      "outputs": [],
      "source": [
        "class Brain(object):\n",
        "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_actions = number_actions\n",
        "        states = Input(shape = (3,))\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        #x = Dropout(rate = 0.1)(x)\n",
        "        x = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        #x = Dropout(rate = 0.1)(x)\n",
        "        q_values = Dense(units = self.number_actions, activation = 'softmax')(x)\n",
        "\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "        self.model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG5jxj9rzF1z"
      },
      "source": [
        "# Implement Deep Q-Learning with Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KhPvqsD0zF10"
      },
      "outputs": [],
      "source": [
        "class DQN(object):\n",
        "\n",
        "    # INITIALIZE ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
        "    def __init__(self, max_memory = 100, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount     # discount factor used in calculating the targets Q\n",
        "\n",
        "    # METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        \"\"\"arguments:\n",
        "        transition: tuple of 4 elemnts (current state, action played, reward received, next state)\n",
        "        game_over : 0 or 1\"\"\"\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]                   # delete first memory element (FIFO)\n",
        "\n",
        "    # CONSTRUCT BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]  # select first elmnt of transition tuple, ie shape of state vector\n",
        "        num_outputs = model.output_shape[-1]\n",
        "\n",
        "        # initialize the batches\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))   # typically batch_size x 3\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs)) # typically batch_size x 5\n",
        "\n",
        "        # extract random transitions from memory and populate input states and outputs Q-values\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            targets[i] = model.predict(current_state)[0]  # predict returns 2 elements, Q-values is first\n",
        "            Q_sa = np.max(targets[i])\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "\n",
        "        return inputs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyesmarlzF10"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6Wuk0yE-zF11",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5bcd89a3-1b33-4017-ce24-e3d85624600c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Environment.__init__() got an unexpected keyword argument 'temperatura_otima'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5a03d082dab4>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# CONSTRUINDO O AMBIENTE CRIANDO UM OBJETO DA CLASSE ENVIRONMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m env = Environment(temperatura_otima=(18.0, 24.0), mes_inicial=0,\n\u001b[0m\u001b[1;32m     17\u001b[0m                   numero_inicial_usuarios=20, taxa_inicial_dados=30)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Environment.__init__() got an unexpected keyword argument 'temperatura_otima'"
          ]
        }
      ],
      "source": [
        "# Configurando sementes para reprodutibilidade\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "\n",
        "# CONFIGURAÇÃO DOS PARÂMETROS\n",
        "epsilon = 0.3   # Taxa de exploração vs. exploração. Aqui, 30% de exploração (seleção aleatória)\n",
        "numero_acoes = 5\n",
        "limite_direcao = (numero_acoes - 1) / 2   # Limite separando direção das ações de mudança de temperatura\n",
        "numero_epocas = 100\n",
        "memoria_maxima = 3000\n",
        "tamanho_batch = 512\n",
        "passo_temperatura = 1.5\n",
        "\n",
        "# CONSTRUINDO O AMBIENTE CRIANDO UM OBJETO DA CLASSE ENVIRONMENT\n",
        "env = Environment(temperatura_otima=(18.0, 24.0), mes_inicial=0,\n",
        "                  numero_inicial_usuarios=20, taxa_inicial_dados=30)\n",
        "\n",
        "# CONSTRUINDO O OBJETO DA REDE NEURAL USANDO A CLASSE BRAIN\n",
        "cerebro = Brain(taxa_aprendizado=0.00001, numero_acoes=numero_acoes)\n",
        "\n",
        "# CONSTRUINDO O MODELO DQN\n",
        "dqn = DQN(memoria_maxima=memoria_maxima, desconto=0.9)\n",
        "\n",
        "# ESCOLHENDO O MODO\n",
        "treinar = True\n",
        "\n",
        "# TREINANDO A IA\n",
        "env.treinar = treinar\n",
        "modelo = cerebro.modelo\n",
        "parada_precoce = True\n",
        "paciencia = 10\n",
        "melhor_recompensa_total = -np.inf\n",
        "contagem_paciencia = 0\n",
        "\n",
        "if env.treinar:\n",
        "    # INICIANDO O LOOP DE TODAS AS ÉPOCAS (1 Época = 5 Meses)\n",
        "    for epoca in range(1, numero_epocas):\n",
        "        # INICIALIZANDO TODAS AS VARIÁVEIS DO AMBIENTE E DO LOOP DE TREINAMENTO\n",
        "        recompensa_total = 0\n",
        "        perda = 0.0\n",
        "        novo_mes = np.random.randint(0, 12)\n",
        "        env.reset(novo_mes=novo_mes)\n",
        "        jogo_terminado = False\n",
        "        estado_atual, _, _ = env.observar()\n",
        "        passo_tempo = 0\n",
        "\n",
        "        # INICIANDO O LOOP DE TODOS OS TIMESTEPS (1 Timestep = 1 Minuto) EM UMA ÉPOCA\n",
        "        while not jogo_terminado and passo_tempo <= 5 * 30 * 24 * 60:\n",
        "            # EXECUTANDO A PRÓXIMA AÇÃO POR EXPLORAÇÃO\n",
        "            if np.random.rand() <= epsilon:   # escolha aleatória dentro de [0,1] abaixo do limite epsilon\n",
        "                acao = np.random.randint(0, numero_acoes)  # ação entre 0 a 4, limite = 2\n",
        "                direcao = -1 if (acao - limite_direcao < 0) else 1\n",
        "                energia_ia = abs(acao - limite_direcao) * passo_temperatura\n",
        "\n",
        "            # EXECUTANDO A PRÓXIMA AÇÃO POR INFERÊNCIA\n",
        "            else:\n",
        "                valores_q = modelo.predict(estado_atual)\n",
        "                acao = np.argmax(valores_q[0])\n",
        "                direcao = -1 if (acao - limite_direcao < 0) else 1\n",
        "                energia_ia = abs(acao - limite_direcao) * passo_temperatura\n",
        "\n",
        "            # ATUALIZANDO O AMBIENTE E ALCANÇANDO O PRÓXIMO ESTADO\n",
        "            proximo_estado, recompensa, jogo_terminado = env.atualizar_env(\n",
        "                direcao, energia_ia, int(passo_tempo / (30 * 24 * 60))\n",
        "            )\n",
        "            recompensa_total += recompensa\n",
        "\n",
        "            # ARMAZENANDO ESSA NOVA TRANSIÇÃO NA MEMÓRIA\n",
        "            dqn.lembrar([estado_atual, acao, recompensa, proximo_estado], jogo_terminado)\n",
        "\n",
        "            # COLETANDO EM DOIS LOTES SEPARADOS OS INPUTS E TARGETS\n",
        "            entradas, alvos = dqn.obter_lote(modelo, tamanho_batch=tamanho_batch)\n",
        "\n",
        "            # COMPUTANDO A PERDA SOBRE OS DOIS LOTES INTEIROS DE INPUTS E TARGETS\n",
        "            perda += modelo.train_on_batch(entradas, alvos)\n",
        "            passo_tempo += 1\n",
        "            estado_atual = proximo_estado  # atualizar o estado atual\n",
        "\n",
        "        # IMPRIMINDO OS RESULTADOS DO TREINAMENTO PARA CADA ÉPOCA\n",
        "        print(f\"Época: {epoca:03d}/{numero_epocas:03d}\")\n",
        "        print(f\"Total de Energia gasta com IA: {env.energia_total_ia:.0f}\")\n",
        "        print(f\"Total de Energia gasta sem IA: {env.energia_total_sem_ia:.0f}\")\n",
        "\n",
        "        # PARADA PRECOCE\n",
        "        if parada_precoce:\n",
        "            if recompensa_total <= melhor_recompensa_total:\n",
        "                contagem_paciencia += 1\n",
        "            elif recompensa_total > melhor_recompensa_total:\n",
        "                melhor_recompensa_total = recompensa_total\n",
        "                contagem_paciencia = 0\n",
        "            if contagem_paciencia >= paciencia:\n",
        "                print(\"Parada Precoce\")\n",
        "                break\n",
        "\n",
        "        # SALVANDO O MODELO\n",
        "        modelo.save(\"modelo.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09hGZVwCzF12"
      },
      "source": [
        "# Evaluating Energy management model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHbLkJojzF12"
      },
      "outputs": [],
      "source": [
        "print('Evaluating one year of energy management...')\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, \\\n",
        "                  initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# LOAD PRE-TRAINED MODEL\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = False\n",
        "\n",
        "# RUNNING 1 YEAR SIMULATION INFERENCE MODE\n",
        "env.train = train\n",
        "current_state, _, _ = env.observe()\n",
        "\n",
        "# STARTING THE LOOP OVER 1 YEAR\n",
        "for timestep in tqdm(range(12 * 30 * 24 * 60)):\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values[0])\n",
        "    if (action - direction_boundary < 0):\n",
        "        direction = -1\n",
        "    else:\n",
        "        direction = 1\n",
        "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "    # UPDATING ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "    next_state, _, _ = env.update_env(direction, energy_ai, \\\n",
        "                                                int(timestep / (30 * 24 * 60)))  # month [0,11]\n",
        "    current_state = next_state    # update the current state\n",
        "\n",
        "# PRINTING THE RESULTS FOR 1 YEAR\n",
        "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "print(\"ENERGY SAVED WITH AI: {:.0f}%\".format((env.total_energy_noai - env.total_energy_ai)/env.total_energy_noai*100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}